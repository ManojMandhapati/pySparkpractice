{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1ce86f-6320-469f-b87d-edec22866a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#UnionByName lets you merge/union 2 DataFrames witha a different number of columns(different schema) by passing allowMissingColumns with the value true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fabc8bec-4db8-41cb-8d32-39f219df24cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "data = [(1,'manoj','male',3000),(2,'megha','female',4000)]\n",
    "data1 = [(2,'megha',4000,'QA'),(3,'Naveen',5000,'SE'),(4,'Manasa',6000,'Payroll')]\n",
    "schema = ['id','name','gender','salary']\n",
    "schema2 = ['id','name','salary','dept']\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df1= spark.createDataFrame(data1,schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e246de3d-c246-4ca0-94e8-c5f5a804a914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n| id|  name|gender| salary|\n+---+------+------+-------+\n|  1| manoj|  male|   3000|\n|  2| megha|female|   4000|\n|  2| megha|  4000|     QA|\n|  3|Naveen|  5000|     SE|\n|  4|Manasa|  6000|Payroll|\n+---+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.union(df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87302f78-e4df-4227-b5f3-cbc5142b6810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+-------+\n| id|  name|gender|salary|   dept|\n+---+------+------+------+-------+\n|  1| manoj|  male|  3000|   null|\n|  2| megha|female|  4000|   null|\n|  2| megha|  null|  4000|     QA|\n|  3|Naveen|  null|  5000|     SE|\n|  4|Manasa|  null|  6000|Payroll|\n+---+------+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#unionByName\n",
    "df.unionByName(df1, allowMissingColumns = True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d52614b-e363-4289-ae63-f3561a0a4a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "unionByName",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
