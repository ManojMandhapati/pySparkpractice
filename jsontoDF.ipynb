{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33ef819a-a8d7-46d9-a8de-02c96b6eb78d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n| ID| Name|Gender|Salary|\n+---+-----+------+------+\n|  1|Manoj|  Male|  3000|\n|  2|Megha|Female|  4000|\n+---+-----+------+------+\n\nroot\n |-- ID: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([ StructField('ID', IntegerType()),\n",
    "                     StructField('Name',StringType()),\n",
    "                     StructField('Gender',StringType()),\n",
    "                    StructField ('Salary', IntegerType())])\n",
    "df = spark.read.json(path = 'dbfs:/FileStore/emp1.json',schema = schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "051b67b2-2ae4-470d-a453-3fadc53ef058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StructType in module pyspark.sql.types:\n\nclass StructType(DataType)\n |  StructType(fields: Optional[List[pyspark.sql.types.StructField]] = None)\n |  \n |  Struct type, consisting of a list of :class:`StructField`.\n |  \n |  This is the data type representing a :class:`Row`.\n |  \n |  Iterating a :class:`StructType` will iterate over its :class:`StructField`\\s.\n |  A contained :class:`StructField` can be accessed by its name or position.\n |  \n |  Examples\n |  --------\n |  >>> from pyspark.sql.types import *\n |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct1[\"f1\"]\n |  StructField('f1', StringType(), True)\n |  >>> struct1[0]\n |  StructField('f1', StringType(), True)\n |  \n |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct1 == struct2\n |  True\n |  >>> struct1 = StructType([StructField(\"f1\", CharType(10), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", CharType(10), True)])\n |  >>> struct1 == struct2\n |  True\n |  >>> struct1 = StructType([StructField(\"f1\", VarcharType(10), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", VarcharType(10), True)])\n |  >>> struct1 == struct2\n |  True\n |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n |  ...     StructField(\"f2\", IntegerType(), False)])\n |  >>> struct1 == struct2\n |  False\n |  \n |  The below example demonstrates how to create a DataFrame based on a struct created\n |  using class:`StructType` and class:`StructField`:\n |  \n |  >>> data = [(\"Alice\", [\"Java\", \"Scala\"]), (\"Bob\", [\"Python\", \"Scala\"])]\n |  >>> schema = StructType([\n |  ...     StructField(\"name\", StringType()),\n |  ...     StructField(\"languagesSkills\", ArrayType(StringType())),\n |  ... ])\n |  >>> df = spark.createDataFrame(data=data, schema=schema)\n |  >>> df.printSchema()\n |  root\n |   |-- name: string (nullable = true)\n |   |-- languagesSkills: array (nullable = true)\n |   |    |-- element: string (containsNull = true)\n |  >>> df.show()\n |  +-----+---------------+\n |  | name|languagesSkills|\n |  +-----+---------------+\n |  |Alice|  [Java, Scala]|\n |  |  Bob|[Python, Scala]|\n |  +-----+---------------+\n |  \n |  Method resolution order:\n |      StructType\n |      DataType\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __getitem__(self, key: Union[str, int]) -> pyspark.sql.types.StructField\n |      Access fields by name or slice.\n |  \n |  __init__(self, fields: Optional[List[pyspark.sql.types.StructField]] = None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __iter__(self) -> Iterator[pyspark.sql.types.StructField]\n |      Iterate the fields\n |  \n |  __len__(self) -> int\n |      Return the number of fields.\n |  \n |  __repr__(self) -> str\n |      Return repr(self).\n |  \n |  add(self, field: Union[str, pyspark.sql.types.StructField], data_type: Union[str, pyspark.sql.types.DataType, NoneType] = None, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None) -> 'StructType'\n |      Construct a :class:`StructType` by adding new elements to it, to define the schema.\n |      The method accepts either:\n |      \n |          a) A single parameter which is a :class:`StructField` object.\n |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n |             metadata(optional). The data_type parameter may be either a String or a\n |             :class:`DataType` object.\n |      \n |      Parameters\n |      ----------\n |      field : str or :class:`StructField`\n |          Either the name of the field or a :class:`StructField` object\n |      data_type : :class:`DataType`, optional\n |          If present, the DataType of the :class:`StructField` to create\n |      nullable : bool, optional\n |          Whether the field to add should be nullable (default True)\n |      metadata : dict, optional\n |          Any additional metadata (default None)\n |      \n |      Returns\n |      -------\n |      :class:`StructType`\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n |      ...     StructField(\"f2\", StringType(), True, None)])\n |      >>> struct1 == struct2\n |      True\n |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n |      >>> struct1 == struct2\n |      True\n |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n |      >>> struct1 == struct2\n |      True\n |  \n |  fieldNames(self) -> List[str]\n |      Returns all field names in a list.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql.types import StringType, StructField, StructType\n |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n |      >>> struct.fieldNames()\n |      ['f1']\n |  \n |  fromInternal(self, obj: Tuple) -> 'Row'\n |      Converts an internal SQL object into a native Python object.\n |  \n |  jsonValue(self) -> Dict[str, Any]\n |  \n |  needConversion(self) -> bool\n |      Does this type needs conversion between Python object and internal SQL object.\n |      \n |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n |  \n |  simpleString(self) -> str\n |  \n |  toInternal(self, obj: Tuple) -> Tuple\n |      Converts a Python object into an internal SQL object.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  fromJson(json: Dict[str, Any]) -> 'StructType' from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from DataType:\n |  \n |  __eq__(self, other: Any) -> bool\n |      Return self==value.\n |  \n |  __hash__(self) -> int\n |      Return hash(self).\n |  \n |  __ne__(self, other: Any) -> bool\n |      Return self!=value.\n |  \n |  json(self) -> str\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from DataType:\n |  \n |  typeName() -> str from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from DataType:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "help(StructType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a751aaaa-ef06-45a9-8e86-aeaa380cc078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n| ID| Name|Gender|Salary|\n+---+-----+------+------+\n|  1|Manoj|  Male|  3000|\n|  2|Megha|Female|  4000|\n+---+-----+------+------+\n\nroot\n |-- ID: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([ StructField('ID', IntegerType()),\n",
    "                     StructField('Name',StringType()),\n",
    "                     StructField('Gender',StringType()),\n",
    "                    StructField ('Salary', IntegerType())])\n",
    "df = spark.read.json(path = 'dbfs:/FileStore/emp2.json',schema = schema, multiLine = True)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f44a27b-c07a-4c96-89f8-bded0bfc4c72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n| ID| Name|Gender|Salary|\n+---+-----+------+------+\n|  3|Manoj|  Male|  3500|\n|  4|Megha|Female|  5000|\n|  1|Manoj|  Male|  3000|\n|  2|Megha|Female|  4000|\n|  1|Manoj|  Male|  3000|\n+---+-----+------+------+\n\nroot\n |-- ID: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType().add(field='ID',data_type=IntegerType())\\\n",
    "                  .add(field = 'Name',data_type = StringType())\\\n",
    "                    .add(field = 'Gender',data_type = StringType())\\\n",
    "                      .add(field = 'Salary',data_type = IntegerType())\n",
    "        \n",
    "df = spark.read.json(path = 'dbfs:/FileStore/*.json', multiLine = True, schema = schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "997c96fe-9a94-4329-884e-49e6cfa859b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "jsontoDF",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
